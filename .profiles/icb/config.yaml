cluster:
  mkdir -p logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --qos={resources.qos}
    --gres=gpu:{resources.gpu}
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name={rule}
    --output=logs/%j-{rule}.out
    --parsable
default-resources:
<<<<<<< Updated upstream
  - partition=gpu_p
  - qos=gpu_normal
  - gpu=1
  - mem_mb=240000
#  - disk_mb=20000
=======
  - partition=cpu_p
  - qos=cpu_normal
  - gpu=0
  - mem_mb=90000
>>>>>>> Stashed changes
restart-times: 0
max-jobs-per-second: 10
max-status-checks-per-second: 1
local-cores: 1
latency-wait: 30
jobs: 20
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
cluster-cancel: scancel
cluster-status: .profiles/slurm-status.py
rerun-triggers:
  - mtime
  - params
  - input
  - software-env
  - code
show-failed-logs: True
