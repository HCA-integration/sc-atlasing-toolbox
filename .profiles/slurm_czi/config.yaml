cluster:
  mkdir -p logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --gpus={resources.gpu}
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name={rule}
    --output=logs/{rule}/%j-{rule}.out
    --parsable
default-resources:
  - partition=cpu
  - gpu=0
  - mem_mb=32000
  - disk_mb=20000
restart-times: 1
max-jobs-per-second: 5
max-status-checks-per-second: 1
local-cores: 1
latency-wait: 30
jobs: 40
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
cluster-cancel: scancel
cluster-status: .profiles/slurm_czi/slurm-status.py
rerun-triggers:
  - mtime
  - params
  - input
  - software-env
  - code
show-failed-logs: True
#shadow-prefix: /hpc/projects/data_lg/michaela.mueller/scratch
shadow-prefix: /tmp/michaela.mueller
latency-wait: 90
configfile: configs/optional/comp_czbiohub.yaml
