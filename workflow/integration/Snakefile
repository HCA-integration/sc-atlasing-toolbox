from pathlib import Path
import pandas as pd

from utils import get_wildcards_from_config, get_params, get_resource, set_defaults


def get_h5ad(wildcards):
    return config['DATASETS'][wildcards.dataset]['adata_file']


config = set_defaults(config)

parameters = pd.read_table(workflow.source_path('params.tsv'))
parameters['output_type'] = parameters['output_type'].str.split(',')
parameters = get_wildcards_from_config(
    config=config,
    config_params=['integration', 'label', 'batch'],
    wildcard_names=['dataset', 'method', 'label', 'batch'],
    explode_by='method',
    
).merge(parameters,on='method')
out_dir = Path(config['output_dir']) / 'integration'


rule run:
    input:
        h5ad=get_h5ad
    output:
        h5ad=out_dir / '{dataset}/{method}.h5ad'
    params:
        dataset=lambda wildcards: get_params(wildcards,parameters,'dataset'),
        batch=lambda wildcards: get_params(wildcards,parameters,'batch'),
        label=lambda wildcards: get_params(wildcards,parameters,'label'),
        output_type=lambda wildcards: get_params(wildcards,parameters,'output_type'),
        env=lambda wildcards: get_params(wildcards,parameters,'env')
    conda:
        lambda wildcards, params: f'envs/{params.env}'
    resources:
        partition=lambda w: get_resource(config,profile=get_params(w,parameters,'resources'),resource_key='partition'),
        qos=lambda w: get_resource(config,profile=get_params(w,parameters,'resources'),resource_key='qos'),
        mem_mb=32000,
    benchmark:
        out_dir / '{dataset}/{method}.benchmark.tsv'
    script:
        'scripts/{wildcards.method}.py'


rule benchmark:
    input:
        benchmark=expand(rules.run.benchmark,zip,**parameters[['dataset', 'method']].to_dict('list'))
    output:
        benchmark=out_dir / 'integration.benchmark.tsv'
    params:
        wildcards=parameters[['dataset', 'method']]
    run:
        benchmark_df = pd.concat([pd.read_table(file) for file in input.benchmark]).reset_index(drop=True)
        benchmark_df = pd.concat([params.wildcards.reset_index(drop=True), benchmark_df],axis=1)
        print(benchmark_df)
        benchmark_df.to_csv(output.benchmark,sep='\t',index=False)


rule all:
    input:
        expand(rules.run.output,zip,**parameters[['dataset', 'method']].to_dict('list')),
        rules.benchmark.output
    default_target: True
