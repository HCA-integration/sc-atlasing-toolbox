from pathlib import Path
import pandas as pd

from utils import get_wildcards_from_config


out_dir = Path(config['output_dir']) / 'metrics'
# TODO: save input pattern in config
in_path = Path(config['output_dir']) / 'integration' / '{dataset}' / '{method}.h5ad'

# get wildcards from input directory
datasets, methods = glob_wildcards(in_path)
integration_wildcards = pd.DataFrame({'dataset': datasets, 'method': methods})

# get full parameters
parameters = get_wildcards_from_config(
    config['DATASETS'],
    ['metrics', 'label', 'batch'],
    ['dataset', 'metric', 'label', 'batch'],
    explode_by='metric'
).merge(
    integration_wildcards,
    on='dataset'
)


rule run:
    message:
        """
        Evaluate {wildcards.metric} on {wildcards.dataset}
        input: {input}
        output: {output}
        wildcards: {wildcards}
        """
    input:
        h5ad=in_path,
        metrics_meta=workflow.source_path('params.tsv')
    output:
        metric=out_dir / '{dataset}/{method}/{metric}.tsv'
    conda:
        '../../envs/scib.yaml'
    script:
        'scripts/{wildcards.metric}.py'


rule merge:
    input:
        expand(
            rules.run.output,
            zip,
            **parameters[['dataset', 'method', 'metric']].to_dict('list')
        )
    output:
        metrics=out_dir / 'metric.tsv'
    run:
        metrics_df = pd.concat([pd.read_table(file) for file in input])
        metrics_df.to_csv(output.metrics,sep='\t',index=False)


rule all:
    input:
        rules.merge.output
    default_target: True
