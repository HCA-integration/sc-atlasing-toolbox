from pathlib import Path
import pandas as pd

from utils import get_wildcards_from_config


out_dir = Path(config['output_dir']) / 'metric'
in_path = Path(config['output_dir']) / 'integration' / '{dataset}' / '{method}.h5ad'

# get wildcards from input directory
datasets, methods = glob_wildcards(in_path)
integration_wildcards = pd.DataFrame({'dataset': datasets, 'method': methods})

# get full parameters
parameters = pd.read_table(workflow.source_path('params.tsv'))
parameters = get_wildcards_from_config(
    config['DATASETS'],
    ['metrics', 'label', 'batch'],
    ['dataset', 'metric', 'label', 'batch'],
    explode_by='metric'
).merge(
    integration_wildcards,
    on='dataset'
)


rule all:
    input: '.snakemake/done.metrics'
    shell:
        """
        rm {input}
        """


rule run:
    message:
        """
        Evaluate {wildcards.metric} on {wildcards.dataset}
        input: {input}
        output: {output}
        wildcards: {wildcards}
        """
    input:
        h5ad=in_path,
        metrics_meta=workflow.source_path('params.tsv')
    output:
        metric=out_dir / '{dataset}/{method}/{metric}.tsv'
    conda:
        '../../envs/scib.yaml'
    script:
        'scripts/{wildcards.metric}.py'


rule merge:
    input:
        expand(
            rules.run.output,
            zip,
            **parameters[['dataset', 'method', 'metric']].to_dict('list')
        )
    output:
        metrics=out_dir / 'metric.tsv',
        done=touch('.snakemake/done.metrics')
    run:
        pd.concat([pd.read_table(file) for file in input]).to_csv(output.metrics)
