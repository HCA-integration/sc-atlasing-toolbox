from pathlib import Path
import pandas as pd

from utils import get_wildcards_from_config, expand_per, get_wildcards


out_dir = Path(config['output_dir']) / 'metrics'
in_path = Path(config['input']['metrics'])

if 'integration' in config['defaults']:
    parameters = get_wildcards_from_config(
        config=config,
        config_params=['integration', 'metrics', 'label', 'batch'],
        wildcard_names=['dataset', 'method', 'metric', 'label', 'batch'],
        explode_by=['method', 'metric']
    )
else:
    # get wildcards from input directory
    datasets, methods = glob_wildcards(in_path)
    integration_wildcards = pd.DataFrame({'dataset': datasets, 'method': methods})
    # get full parameters
    parameters = get_wildcards_from_config(
        config=config,
        config_params=['metrics', 'label', 'batch'],
        wildcard_names=['dataset', 'metric', 'label', 'batch'],
        explode_by='metric'
    ).merge(
        integration_wildcards,
        on='dataset'
    )


rule run:
    message:
        """
        Evaluate {wildcards.metric} on {wildcards.dataset}
        input: {input}
        output: {output}
        wildcards: {wildcards}
        """
    input:
        h5ad=in_path,
        metrics_meta=workflow.source_path('params.tsv')
    output:
        metric=out_dir / 'datasets/{dataset}/{method}/{metric}.tsv'
    conda:
        '../../envs/scib.yaml'
    benchmark:
        out_dir / 'datasets/{dataset}/{method}/{metric}.benchmark.tsv'
    script:
        'scripts/{wildcards.metric}.py'


rule merge:
    input:
        metrics=lambda wildcards: expand_per(rules.run.output,parameters,wildcards,['dataset', 'method', 'metric']),
        benchmark=lambda wildcards: expand_per(rules.run.benchmark,parameters,wildcards,['dataset', 'method',
                                                                                         'metric']),
    output:
        metrics=out_dir / 'metrics.tsv',
        plot=out_dir / 'metrics.png',
        time=out_dir / 'metrics_time.png',
    params:
        wildcards=get_wildcards(parameters,['dataset', 'method', 'metric'])
    conda: '../../envs/scanpy.yaml'
    script: 'scripts/merge.py'


use rule merge as merge_per_dataset with:
    input:
        metrics=lambda wildcards: expand_per(rules.run.output,parameters,wildcards,['method', 'metric']),
        benchmark=lambda wildcards: expand_per(rules.run.benchmark,parameters,wildcards,['method', 'metric']),
    output:
        metrics=out_dir / 'per_dataset' / '{dataset}_metrics.tsv',
        plot=out_dir / 'per_dataset' / '{dataset}.png',
        time=out_dir / 'per_dataset' / '{dataset}_time.png',
    params:
        wildcards=lambda wildcards: get_wildcards(parameters,['method', 'metric'],wildcards)


use rule merge as merge_per_method with:
    input:
        metrics=lambda wildcards: expand_per(rules.run.output,parameters,wildcards,['dataset', 'metric']),
        benchmark=lambda wildcards: expand_per(rules.run.benchmark,parameters,wildcards,['dataset', 'metric']),
    output:
        metrics=out_dir / 'per_method' / '{method}.tsv',
        plot=out_dir / 'per_method' / '{method}.png',
        time=out_dir / 'per_method' / '{method}_time.png',
    params:
        wildcards=lambda wildcards: get_wildcards(parameters,['dataset', 'metric'],wildcards)


rule all:
    input:
        expand(rules.merge_per_dataset.output,zip,**get_wildcards(parameters,['dataset'])),
        expand(rules.merge_per_method.output,zip,**get_wildcards(parameters,['method'])),
        rules.merge.output,
    default_target: True
