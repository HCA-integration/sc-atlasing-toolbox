"""
Data Loader
"""
from pathlib import Path
import pandas as pd

from scripts.utils import get_url
from utils.data import load_dataset_df
from utils.misc import all_but, unlist_dict
from utils.config import get_resource
from utils.wildcards import get_wildcards


module_name = 'load_data'

if 'dataset_df' in config.keys():
    dataset_df = config['dataset_df']
else:
    dataset_df = load_dataset_df(config['dataset_meta'])
out_dir = Path(config['output_dir']) / module_name

# set default filter settings per values per study filters
if 'filter_per_organ' not in config.keys():
    config['filter_per_organ'] = {}

for _, row in dataset_df[['study', 'organ']].drop_duplicates().iterrows():
    study, organ = row[['study', 'organ']]

    if organ not in config['filter_per_organ'].keys():
        config['filter_per_organ'][organ] = {}
    if 'filter_per_study' not in config.keys():
        config['filter_per_study'] = {}
    if study not in config['filter_per_study'].keys():
        config['filter_per_study'][study] = {}

    # use organ settings as default for all studies of that organ
    for key, value in config['filter_per_organ'][organ].items():
        if key not in config['filter_per_study'][study]:
            config['filter_per_study'][study][key] = value

wildcard_constraints:
    dataset='\w+',
    study='\w+',
    organ='\w+',


module common:
    snakefile: "../common/Snakefile"
    config: config

use rule * from common as common_ *


rule download:
    """
    Download files
    TODO: better warning message
    """
    output:
        h5ad=out_dir / 'download' / '{dataset}.h5ad'
    params:
        dataset_df=dataset_df
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/download.py'

datasets_to_download = dataset_df[
    dataset_df['url'].apply(lambda x: not Path(x).is_file())
]['dataset']
rule download_all:
    input:
        expand(rules.download.output, dataset=datasets_to_download)


def get_h5ad(wildcards):
    file_path = dataset_df[dataset_df['dataset'] == wildcards.dataset].astype(str).reset_index()['url'][0]
    if Path(str(file_path)).exists():
        return file_path
    return rules.download.output.h5ad


rule metadata:
    input:
        h5ad=get_h5ad,
        schema=config["schema_file"]
    params:
        meta=lambda wildcards: unlist_dict(
            get_wildcards(dataset_df,columns=all_but(dataset_df.columns,'subset'),wildcards=wildcards)
        )
    output:
        zarr=directory(out_dir / 'processed/' / '{dataset}.zarr'),
        plot=out_dir / 'processed/counts/{dataset}.png',
    conda:
        '../../envs/scanpy.yaml'
    resources:
        mem_mb=get_resource(config,profile='cpu',resource_key='mem_mb'),
        disk_mb=20000,
    shadow: 'shallow'
    script:
        'scripts/metadata.py'


rule metadata_all:
    input:
        expand(rules.metadata.output,dataset=dataset_df['dataset'])


rule merge_study:
    """
    Merge datasets that had to be split
    """
    input:
        lambda wildcards: expand(
            rules.metadata.output.zarr,
            dataset=get_wildcards(dataset_df,'dataset',wildcards)['dataset']
        )
    output:
        zarr=directory(out_dir / 'merged' / 'study' / '{study}.zarr'),
    params:
        dataset=lambda wildcards: wildcards.study,
        merge_strategy='inner'
    resources:
        mem_mb=get_resource(config,profile='cpu',resource_key='mem_mb'),
        disk_mb=20000,
    conda:
        '../../envs/scanpy.yaml'
    # shadow: 'minimal'
    script:
        'scripts/merge.py'


rule merge_study_all:
    input: expand(rules.merge_study.output,**get_wildcards(dataset_df,['study']))


rule filter:
    """
    QC filter per study
    TODO: keep filtered out cells for stats
    """
    input:
        zarr=rules.merge_study.output.zarr
    output:
        zarr=directory(out_dir / 'filtered' / '{study}.zarr'),
        removed=directory(out_dir / 'filtered' / 'removed' / '{study}.zarr'),
    params:
        filter=lambda wildcards: config['filter_per_study'][wildcards.study]
    resources:
        mem_mb=get_resource(config,profile='cpu',resource_key='mem_mb'),
        disk_mb=20000,
    conda:
        '../../envs/scanpy.yaml'
    # shadow: 'minimal'
    script:
        'scripts/filter.py'


rule filter_all:
    input: expand(rules.filter.output,**get_wildcards(dataset_df,['study']))


rule merge_organ:
    input:
        lambda wildcards: expand(
            rules.filter.output.zarr,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        zarr=directory(out_dir / 'merged' / 'organ' / '{organ}.zarr')
    params:
        dataset=lambda wildcards: wildcards.organ,
        merge_strategy='inner'
    resources:
        mem_mb=get_resource(config,profile='cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,profile='cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    # shadow: 'minimal'
    script:
        'scripts/merge.py'


rule merge_organ_filter:
    input:
        lambda wildcards: expand(
            rules.filter.output.removed,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        zarr=directory(out_dir / 'merged' / 'organ' / 'filtered' / '{organ}.zarr')
    params:
        dataset=lambda wildcards: wildcards.organ,
        merge_strategy='outer'
    resources:
        mem_mb=get_resource(config,profile='cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,profile='cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    # shadow: 'minimal'
    script:
        'scripts/merge.py'


rule merge_subset:
    input:
        lambda wildcards: expand(
            rules.filter.output.zarr,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        zarr=directory(out_dir / 'merged' / 'subset' / '{organ}-{subset}.zarr')
    params:
        dataset=lambda wildcards: f'{wildcards.organ}-{wildcards.subset}',
        merge_strategy='inner'
    resources:
        mem_mb=get_resource(config,profile='cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,profile='cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    # shadow: 'minimal'
    script:
        'scripts/merge.py'


rule  merge_subset_all:
    input:
        expand(
            rules.merge_subset.output,
            **get_wildcards(dataset_df,['organ', 'subset'],drop_na=True)
        )

include: 'rules/metadata.smk'

rule all:
    input:
        expand(rules.merge_organ.output,**get_wildcards(dataset_df,['organ'])),
        expand(rules.merge_organ_filter.output,**get_wildcards(dataset_df,['organ'])),
    default_target: True


rule dependency_graph:
    input:
        expand(
            rules.common_dependency_graph.input,
            images=config['images'] + f'/{module_name}',
            target='all'
        )
