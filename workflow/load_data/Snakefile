"""
Data Loader
"""
from pathlib import Path

from scripts.utils import get_from_dataset, get_url, unlist_dict
from utils import get_wildcards, get_resource, load_dataset_df


def load_dataset_df(file):
    dataset_df = pd.read_table(file,comment='#')
    # dataset_df['subset'] = dataset_df['subset'].str.split(',')
    return dataset_df


module_name = 'load_data'
dataset_df = load_dataset_df(workflow.source_path(config['dataset_meta']))
out_dir = Path(config['output_dir']) / module_name

# set default filter settings per values per study filters
for _, row in dataset_df[['study', 'organ']].drop_duplicates().iterrows():
    study, organ = row[['study', 'organ']]

    if organ not in config['filter_per_organ'].keys():
        config['filter_per_organ'][organ] = {}
    if 'filter_per_study' not in config.keys():
        config['filter_per_study'] = {}
    if study not in config['filter_per_study'].keys():
        config['filter_per_study'][study] = {}

    # use organ settings as default for all studies of that organ
    for key, value in config['filter_per_organ'][organ].items():
        if key not in config['filter_per_study'][study]:
            config['filter_per_study'][study][key] = value

wildcard_constraints:
    dataset='\w+',
    study='\w+',
    organ='\w+',


module common:
    snakefile: "../common/Snakefile"
    config: config

use rule * from common as common_ *


rule download:
    output:
        h5ad=out_dir / 'download/{dataset}.h5ad'
    run:
        url = get_url(dataset_df,wildcards)
        shell("wget -O {output} '{url}'")


rule download_all:
    input:
        expand(rules.download.output,dataset=dataset_df['dataset'])


rule metadata:
    input:
        h5ad=rules.download.output.h5ad
    params:
        meta=lambda wildcards: unlist_dict(get_wildcards(dataset_df,wildcards=wildcards))
    output:
        h5ad=out_dir / 'processed/{dataset}.h5ad',
        plot=out_dir / 'processed/counts/{dataset}.png',
    conda:
        '../../envs/scanpy.yaml'
    resources:
        mem_mb=get_resource(config,'cpu',resource_key='mem_mb'),
        disk_mb=20000,
    script:
        'scripts/metadata.py'


rule metadata_all:
    input:
        expand(rules.metadata.output,dataset=dataset_df['dataset'])


rule merge_study:
    """
    Merge datasets that had to be split
    """
    input:
        lambda wildcards: expand(
            rules.metadata.output.h5ad,
            dataset=get_wildcards(dataset_df,'dataset',wildcards)['dataset']
        )
    output:
        h5ad=out_dir / 'merged/{study}.h5ad'
    params:
        dataset=lambda wildcards: wildcards.study
    resources:
        mem_mb=get_resource(config,'cpu',resource_key='mem_mb'),
        disk_mb=20000,
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule merge_study_all:
    input: expand(rules.merge_study.output,**get_wildcards(dataset_df,['study']))


rule filter:
    """
    QC filter per study
    TODO: keep filtered out cells for stats
    """
    input:
        h5ad=rules.merge_study.output.h5ad
    output:
        h5ad=out_dir / 'filtered' / '{study}.h5ad',
        removed=out_dir / 'filtered' / 'removed' / '{study}.h5ad',
    params:
        filter=lambda wildcards: config['filter_per_study'][wildcards.study]
    resources:
        mem_mb=get_resource(config,'cpu',resource_key='mem_mb'),
        disk_mb=20000,
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/filter.py'


rule filter_all:
    input: expand(rules.filter.output,**get_wildcards(dataset_df,['study']))


def get_datasets_per_organ(wildcards):
    targets = []
    studies = get_from_dataset('organ',wildcards.organ,'study').unique()
    for study in studies:
        datasets_per_study = get_from_dataset('study',study,'dataset').to_list()
        if len(datasets_per_study) == 1:
            targets.extend(expand(rules.metadata.output.h5ad,dataset=datasets_per_study))
        else:
            targets.extend(expand(rules.merge_study.output.h5ad,study=study))
    return targets


rule merge_organ:
    input:
        lambda wildcards: expand(
            rules.filter.output.h5ad,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        h5ad=out_dir / 'merged' / 'organ' / '{organ}.h5ad'
    params:
        dataset=lambda wildcards: wildcards.organ
    resources:
        mem_mb=get_resource(config,'cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,'cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule merge_organ_filter:
    input:
        lambda wildcards: expand(
            rules.filter.output.removed,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        h5ad=out_dir / 'merged' / 'organ' / 'filtered' / '{organ}.h5ad'
    params:
        dataset=lambda wildcards: wildcards.organ
    resources:
        mem_mb=get_resource(config,'cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,'cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule all:
    input:
        expand(rules.merge_organ.output,**get_wildcards(dataset_df,['organ'])),
        expand(rules.merge_organ_filter.output,**get_wildcards(dataset_df,['organ'])),
    default_target: True


rule dependency_graph:
    input:
        expand(
            rules.common_dependency_graph.input,
            images=config['images'] + f'/{module_name}',
            target='all'
        )
