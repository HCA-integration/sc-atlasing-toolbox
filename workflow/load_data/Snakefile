"""
Data Loader
"""
from pathlib import Path
import pandas as pd

from scripts.utils import get_from_dataset, get_url, unlist_dict

module_name = 'load_data'
dataset_df = pd.read_table(workflow.source_path(config['dataset_meta']),comment='#')
out_dir = Path(config['output_dir']) / module_name

# set default filter settings per values per study filters
for _, row in dataset_df[['study', 'organ']].drop_duplicates().iterrows():
    study, organ = row[['study', 'organ']]

    if organ not in config['filter_per_organ'].keys():
        config['filter_per_organ'][organ] = {}
    if 'filter_per_study' not in config.keys():
        config['filter_per_study'] = {}
    if study not in config['filter_per_study'].keys():
        config['filter_per_study'][study] = {}

    # use organ settings as default for all studies of that organ
    for key, value in config['filter_per_organ'][organ].items():
        if key not in config['filter_per_study'][study]:
            config['filter_per_study'][study][key] = value

wildcard_constraints:
    dataset='\w+',
    study='\w+',
    organ='\w+',


module common:
    snakefile: "../common/Snakefile"
    config: config

use rule * from common as common_ *


rule download:
    output:
        h5ad=out_dir / 'download/{dataset}.h5ad'
    run:
        url = get_url(wildcards)
        shell("wget -O {output} '{url}'")


rule download_all:
    input:
        expand(rules.download.output,dataset=dataset_df['dataset'])


rule metadata:
    input:
        h5ad=rules.download.output.h5ad
    params:
        meta=lambda wildcards: unlist_dict(
            get_from_dataset(dataset_df,'dataset',wildcards.dataset).to_dict('list')
        )
    output:
        h5ad=out_dir / 'processed/{dataset}.h5ad',
        plot=out_dir / 'processed/counts/{dataset}.png',
    conda:
        '../../envs/scanpy.yaml'
    resources:
        mem_mb=200000,
        disk_mb=20000,
    script:
        'scripts/metadata.py'


rule metadata_all:
    input:
        expand(rules.metadata.output,dataset=dataset_df['dataset'])


rule merge_study:
    """
    Merge datasets that had to be split
    """
    input:
        lambda wildcards: expand(
            rules.metadata.output.h5ad,
            dataset=get_from_dataset(dataset_df,'study',wildcards.study,'dataset')
        )
    output:
        h5ad=out_dir / 'merged/{study}.h5ad'
    params:
        dataset=lambda wildcards: wildcards.study
    resources:
        mem_mb=200000,
        disk_mb=20000
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule filter:
    """
    QC filter per study
    """
    input:
        h5ad=rules.merge_study.output.h5ad
    output:
        h5ad=out_dir / 'filtered' / '{study}.h5ad'
    params:
        filter=lambda wildcards: config['filter_per_study'][wildcards.study]
    resources:
        mem_mb=200000,
        disk_mb=20000
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/filter.py'


def get_datasets_per_organ(wildcards):
    targets = []
    studies = get_from_dataset('organ',wildcards.organ,'study').unique()
    for study in studies:
        datasets_per_study = get_from_dataset('study',study,'dataset').to_list()
        if len(datasets_per_study) == 1:
            targets.extend(expand(rules.metadata.output.h5ad,dataset=datasets_per_study))
        else:
            targets.extend(expand(rules.merge_study.output.h5ad,study=study))
    return targets


rule merge_organ:
    input:
        lambda wildcards: expand(
            rules.filter.output.h5ad,
            study=dataset_df[dataset_df['organ'] == wildcards.organ]['study'].unique(),
        )
    output:
        h5ad=out_dir / 'merged' / 'organ' / '{organ}.h5ad'
    params:
        dataset=lambda wildcards: wildcards.organ
    resources:
        mem_mb=500000,
        disk_mb=100000
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule all:
    input:
        rules.metadata_all.input,
        expand(rules.merge_study.output,study=dataset_df['study'].unique()),
        expand(rules.filter.output,study=dataset_df['study'].unique()),
        expand(rules.merge_organ.output,organ=dataset_df['organ'].unique()),
    default_target: True


rule dependency_graph:
    input:
        expand(
            rules.common_dependency_graph.input,
            images=config['images'] + f'/{module_name}',
            target='all'
        )
