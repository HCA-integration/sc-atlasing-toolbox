"""
Data Loader
"""
from pathlib import Path
import pandas as pd
import requests


def get_from_dataset(key, value, column=None, debug=False):
    if column is None:
        column = dataset_df.columns
    sub = dataset_df.query(f'{key} == @value')
    if debug:
        print(f'##DEBUG## {key, value, column}')
        print(sub)
        print(sub[column])
    return sub[column]


def get_url(wildcards):
    url = get_from_dataset('dataset',wildcards.dataset,'url').iloc[0]
    if not isinstance(url,str):
        # infer CxG URL from collection and dataset IDs
        collection_id = get_from_dataset('dataset',wildcards.dataset,'collection_id').iloc[0]
        dataset_id = get_from_dataset('dataset',wildcards.dataset,'dataset_id').iloc[0]
        url = f'https://api.cellxgene.cziscience.com/curation/v1/collections/{collection_id}/datasets/{dataset_id}/assets/'
        assets = requests.get(url=url).json()
        asset = [a for a in assets if a["filetype"] == "H5AD"][0]
        url = asset["presigned_url"]
    return url


def unlist_dict(dictionary):
    return {
        k: v[0] if isinstance(v,list) and len(v) == 1 else v
        for k, v in dictionary.items()
    }


module_name = 'load_data'
dataset_df = pd.read_table(workflow.source_path(config['dataset_meta']),comment='#')
out_dir = Path(config['output_dir']) / module_name

wildcard_constraints:
    dataset='\w+',
    study='\w+',
    organ='\w+',


module common:
    snakefile: "../common/Snakefile"
    config: config

use rule * from common as common_ *


rule download:
    output:
        h5ad=out_dir / 'download/{dataset}.h5ad'
    run:
        url = get_url(wildcards)
        shell("wget -O {output} '{url}'")


rule download_all:
    input:
        expand(rules.download.output,dataset=dataset_df['dataset'])


rule metadata:
    input:
        h5ad=rules.download.output.h5ad
    params:
        meta=lambda wildcards: unlist_dict(
            get_from_dataset('dataset',wildcards.dataset).to_dict('list')
        )
    output:
        h5ad=out_dir / 'processed/{dataset}.h5ad',
        plot=out_dir / 'processed/{dataset}_counts.png',
    conda:
        '../../envs/scanpy.yaml'
    resources:
        mem_mb=200000,
        disk_mb=20000,
    script:
        'scripts/metadata.py'


rule metadata_all:
    input:
        expand(rules.metadata.output,dataset=dataset_df['dataset'])


rule merge_study:
    input:
        lambda wildcards: expand(
            rules.metadata.output.h5ad,
            dataset=get_from_dataset('study',wildcards.study,'dataset')
        )
    output:
        h5ad=out_dir / 'merged/{study}.h5ad'
    params:
        dataset=lambda wildcards: wildcards.study
    resources:
        mem_mb=200000,
        disk_mb=20000
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


def get_datasets_per_organ(wildcards):
    targets = []
    studies = get_from_dataset('organ',wildcards.organ,'study').unique()
    for study in studies:
        datasets_per_study = get_from_dataset('study',study,'dataset').to_list()
        if len(datasets_per_study) == 1:
            targets.extend(expand(rules.metadata.output.h5ad,dataset=datasets_per_study))
        else:
            targets.extend(expand(rules.merge_study.output.h5ad,study=study))
    return targets


rule merge_organ:
    input: expand(rules.merge_study.output.h5ad,study=dataset_df['study'].unique())
    output:
        h5ad=out_dir / 'merged/organ/{organ}.h5ad'
    params:
        dataset=lambda wildcards: wildcards.organ
    resources:
        mem_mb=500000,
        disk_mb=100000
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule all:
    input:
        rules.metadata_all.input,
        expand(rules.merge_study.output, study=dataset_df['study'].unique()),
        expand(rules.merge_organ.output,organ=dataset_df['organ'].unique()),
    default_target: True


rule dependency_graph:
    input:
        expand(
            rules.common_dependency_graph.input,
            images=config['images'] + f'/{module_name}',
            target='all'
        )
