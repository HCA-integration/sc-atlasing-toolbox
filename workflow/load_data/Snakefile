"""
Data Loader
"""
from pathlib import Path
import pandas as pd

from scripts.utils import get_url, unlist_dict
from utils import all_but, get_wildcards, get_resource


def load_dataset_df(file):
    df = pd.read_table(file,comment='#')
    df['subset'] = df['subset'].str.split(',')
    df = df.explode('subset')
    return df


module_name = 'load_data'
dataset_df = load_dataset_df(workflow.source_path(config['dataset_meta']))
out_dir = Path(config['output_dir']) / module_name

# set default filter settings per values per study filters
if 'filter_per_organ' not in config.keys():
    config['filter_per_organ'] = {}

for _, row in dataset_df[['study', 'organ']].drop_duplicates().iterrows():
    study, organ = row[['study', 'organ']]

    if organ not in config['filter_per_organ'].keys():
        config['filter_per_organ'][organ] = {}
    if 'filter_per_study' not in config.keys():
        config['filter_per_study'] = {}
    if study not in config['filter_per_study'].keys():
        config['filter_per_study'][study] = {}

    # use organ settings as default for all studies of that organ
    for key, value in config['filter_per_organ'][organ].items():
        if key not in config['filter_per_study'][study]:
            config['filter_per_study'][study][key] = value

wildcard_constraints:
    dataset='\w+',
    study='\w+',
    organ='\w+',


module common:
    snakefile: "../common/Snakefile"
    config: config

use rule * from common as common_ *


rule download:
    output:
        h5ad=out_dir / 'download/{dataset}.h5ad'
    run:
        url = get_url(dataset_df,wildcards)
        shell("wget -O {output} '{url}'")


rule download_all:
    input:
        expand(rules.download.output,dataset=dataset_df['dataset'])


rule metadata:
    input:
        h5ad=rules.download.output.h5ad
    params:
        meta=lambda wildcards: unlist_dict(
            get_wildcards(dataset_df,columns=all_but(dataset_df.columns,'subset'),wildcards=wildcards)
        )
    output:
        zarr=directory(out_dir / 'processed/' / '{dataset}.zarr'),
        plot=out_dir / 'processed/counts/{dataset}.png',
    conda:
        '../../envs/scanpy.yaml'
    resources:
        mem_mb=get_resource(config,'cpu',resource_key='mem_mb'),
        disk_mb=20000,
    script:
        'scripts/metadata.py'


rule metadata_all:
    input:
        expand(rules.metadata.output,dataset=dataset_df['dataset'])


rule merge_study:
    """
    Merge datasets that had to be split
    """
    input:
        lambda wildcards: expand(
            rules.metadata.output.zarr,
            dataset=get_wildcards(dataset_df,'dataset',wildcards)['dataset']
        )
    output:
        zarr=directory(out_dir / 'merged' / '{study}.zarr'),
    params:
        dataset=lambda wildcards: wildcards.study
    resources:
        mem_mb=get_resource(config,'cpu',resource_key='mem_mb'),
        disk_mb=20000,
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule merge_study_all:
    input: expand(rules.merge_study.output,**get_wildcards(dataset_df,['study']))


rule filter:
    """
    QC filter per study
    TODO: keep filtered out cells for stats
    """
    input:
        zarr=rules.merge_study.output.zarr
    output:
        zarr=directory(out_dir / 'filtered' / '{study}.zarr'),
        removed=directory(out_dir / 'filtered' / 'removed' / '{study}.zarr'),
    params:
        filter=lambda wildcards: config['filter_per_study'][wildcards.study]
    resources:
        mem_mb=get_resource(config,'cpu',resource_key='mem_mb'),
        disk_mb=20000,
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/filter.py'


rule filter_all:
    input: expand(rules.filter.output,**get_wildcards(dataset_df,['study']))


rule merge_organ:
    input:
        lambda wildcards: expand(
            rules.filter.output.zarr,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        zarr=directory(out_dir / 'merged' / 'organ' / '{organ}.zarr')
    params:
        dataset=lambda wildcards: wildcards.organ
    resources:
        mem_mb=get_resource(config,'cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,'cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule merge_organ_filter:
    input:
        lambda wildcards: expand(
            rules.filter.output.removed,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        zarr=directory(out_dir / 'merged' / 'organ' / 'filtered' / '{organ}.zarr')
    params:
        dataset=lambda wildcards: wildcards.organ
    resources:
        mem_mb=get_resource(config,'cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,'cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule merge_subset:
    input:
        lambda wildcards: expand(
            rules.filter.output.zarr,
            **get_wildcards(dataset_df,['study'],wildcards),
        ),
    output:
        zarr=directory(out_dir / 'merged' / 'subset' / '{organ}-{subset}.zarr')
    params:
        dataset=lambda wildcards: wildcards
    resources:
        mem_mb=get_resource(config,'cpu_merged',resource_key='mem_mb'),
        disk_mb=get_resource(config,'cpu_merged',resource_key='disk_mb'),
    threads:
        dataset_df['dataset'].nunique()
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/merge.py'


rule  merge_subset_all:
    input:
        expand(
            rules.merge_subset.output,
            **get_wildcards(dataset_df,['organ', 'subset'],drop_na=True)
        )


rule zarr_to_h5ad:
    input:
        zarr=out_dir / '{file}.zarr'
    output:
        h5ad=out_dir / '{file}.h5ad'
    conda:
        '../../envs/scanpy.yaml'
    script:
        'scripts/convert_zarr_h5ad.py'


rule all:
    input:
        expand(rules.merge_organ.output,**get_wildcards(dataset_df,['organ'])),
        expand(rules.merge_organ_filter.output,**get_wildcards(dataset_df,['organ'])),
    default_target: True


rule dependency_graph:
    input:
        expand(
            rules.common_dependency_graph.input,
            images=config['images'] + f'/{module_name}',
            target='all'
        )
