"""
Data Loader
"""
from pathlib import Path
import pandas as pd
import requests


def get_from_dataset(key, value, column=None, debug=False):
    if column is None:
        column = dataset_df.columns
    sub = dataset_df.query(f'{key} == @value')
    if debug:
        print(f'##DEBUG## {key, value, column}')
        print(sub)
        print(sub[column])
    return sub[column]


def get_url(wildcards):
    url = get_from_dataset('dataset_name',wildcards.dataset,'url').iloc[0]
    if not isinstance(url, str):
        # infer CxG URL from collection and dataset IDs
        collection_id = get_from_dataset('dataset_name',wildcards.dataset,'collection_id').iloc[0]
        dataset_id = get_from_dataset('dataset_name',wildcards.dataset,'dataset_id').iloc[0]
        url = f'https://api.cellxgene.cziscience.com/curation/v1/collections/{collection_id}/datasets/{dataset_id}/assets/'
        assets = requests.get(url=url).json()
        asset = [a for a in assets if a["filetype"] == "H5AD"][0]
        url = asset["presigned_url"]
    return url


def unlist_dict(dictionary):
    return {
        k: v[0] if isinstance(v, list) and len(v) == 1 else v
        for k, v in dictionary.items()
    }


dataset_df = pd.read_table(workflow.source_path(config['dataset_meta']), comment='#')
out_dir = Path(config['output_dir']) / 'load_data'


rule download:
    output:
        h5ad=out_dir / 'download/{dataset}.h5ad'
    params:
        url=get_url
    shell:
        """
        wget -O {output} "{params.url}"
        """


rule download_all:
    input:
        expand(rules.download.output,dataset=dataset_df['dataset_name'])


rule metadata:
    input:
        h5ad=rules.download.output.h5ad
    params:
        meta=lambda wildcards: unlist_dict(
            get_from_dataset('dataset_name', wildcards.dataset).to_dict('list')
        )
    output:
        h5ad=out_dir / 'processed/{dataset}.h5ad',
        plot=out_dir / 'processed/{dataset}_counts.png',
    conda:
        '../../envs/scanpy.yaml'
    resources:
        mem_mb=200000,
        disk_mb=20000,
    script:
        'scripts/metadata.py'


rule metadata_all:
    input:
        expand(rules.metadata.output,dataset=dataset_df['dataset_name'])


rule merge:
    input:
        lambda wildcards: expand(
            rules.metadata.output.h5ad,
            dataset=get_from_dataset('organ', wildcards.organ, 'dataset_name')
        )
    output:
        h5ad=out_dir / 'merged/{organ}.h5ad'
    conda:
        '../../envs/scanpy.yaml'
    resources:
        mem_mb=500000,
        disk_mb=100000
    script:
        'scripts/merge.py'


rule all:
    input:
        rules.download_all.input,
        rules.metadata_all.input,
        expand(rules.merge.output,organ=dataset_df['organ'].unique()),
    default_target: True
