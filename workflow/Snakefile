from pprint import pprint
from pathlib import Path
import pandas as pd
from snakemake.utils import min_version

from utils.pipeline import update_module_configs, config_for_module, update_input_files_per_dataset
from utils.ModuleConfig import ModuleConfig
from integration.IntegrationConfig import IntegrationConfig


min_version("6.0")
container: "docker://condaforge/mambaforge:latest"

configfile: "configs/outputs.yaml"
configfile: "configs/load_data/config.yaml"
configfile: "configs/exploration/config.yaml"

# params = pd.read_table('configs/modules.tsv',comment='#')
# params['submodules'] = params['submodules'].str.split(',')
# config = update_module_configs(config, params)

config_kwargs = {
    'clustering': dict(
        config_params=['neighbors_key', 'resolutions'],
        wildcard_names=['neighbors_key', 'resolution'],
        rename_config_params={'resolutions': 'resolution'},
        explode_by='resolution',
    ),
    'integration': dict(
        parameters=workflow.source_path('integration/params.tsv'),
        config_params=['methods', 'batch', 'label', 'norm_counts', 'raw_counts'],
        wildcard_names=['method', 'batch', 'label'],
        rename_config_params={'methods': 'method'},
        explode_by=['method', 'batch'],
    ),
    'metrics': dict(
        parameters=workflow.source_path('metrics/params.tsv'),
        config_params=['label', 'batch', 'methods', 'unintegrated'],
        wildcard_names=['label', 'batch', 'metric'],
        rename_config_params={'methods': 'metric'},
        explode_by=['metric', 'batch'],
        paramspace_kwargs=dict(filename_params=['label', 'batch', 'metric'], filename_sep='--'),
    ),
}

config_classes = {
    'integration': IntegrationConfig,
}

config['DATASETS'] = config.get('DATASETS', {})
default_datasets = config.get('defaults', {}).get('datasets', config['DATASETS'].keys())
for dataset, dataset_config in config['DATASETS'].items():
    for module_name in dataset_config.get('input', {}).keys():
        if dataset not in default_datasets:
            continue
        config = update_input_files_per_dataset(
            dataset=dataset,
            module_name=module_name,
            config=config,
            config_class_map=config_classes,
            config_kwargs=config_kwargs,
        )


# TODO move to data loader and exploration
config['dataset_meta'] = Path(config['dataset_meta']).resolve()

os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'

envvars:
    'HDF5_USE_FILE_LOCKING'

# Import modules
module common:
    snakefile: "common/Snakefile"
    config: config

module load_data:
    snakefile: "load_data/Snakefile"
    config: config

module exploration:
    snakefile: "exploration/Snakefile"
    config: config

module subset:
    snakefile: "subset/Snakefile"
    config: config

module preprocessing:
    snakefile: "preprocessing/Snakefile"
    config: config

module clustering:
    snakefile: "clustering/Snakefile"
    config: config

module label_transfer:
    snakefile: "label_transfer/Snakefile"
    config: config

module label_harmonization:
    snakefile: "label_harmonization/Snakefile"
    config: config_for_module(config, module='label_harmonization')

module relabel:
    snakefile: "relabel/Snakefile"
    config: config

module integration:
    snakefile: "integration/Snakefile"
    config: config

# module integration_per_lineage:
#     snakefile: "integration_per_lineage/Snakefile"
#     config: config

module metrics:
    snakefile: "metrics/Snakefile"
    config: config

use rule * from common as common_ *
use rule * from load_data as load_data_ *
use rule * from preprocessing as preprocessing_ *
use rule * from clustering as clustering_ *
use rule * from exploration as exploration_ *
use rule * from subset as subset_ *
use rule * from label_transfer as label_transfer_ *
use rule * from label_harmonization as label_harmonization_ *
use rule * from relabel as relabel_ *
use rule * from integration as integration_ *
# use rule * from integration_per_lineage as integration_per_lineage_ *
use rule * from metrics as metrics_ *


rule all:
    input: rules.dependency_graph.input
    default_target: True


modules = [
    'load_data',
    'exploration',
    'subset',
    'integration',
    'metrics',
    'preprocessing',
    'label_transfer',
    'label_harmonization'
]

rule dependency_graph:
    input:
        expand(
            rules.common_dependency_graph.input,
            images=config['images'],
            target=[f'{_module}_all' for _module in modules] + ['all']
        )
