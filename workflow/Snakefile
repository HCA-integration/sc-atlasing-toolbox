from pprint import pprint
from pathlib import Path
import pandas as pd
from snakemake.utils import min_version

from utils.pipeline import update_module_configs, config_for_module, update_input_files_per_dataset, update_file_for_module_param
from utils.ModuleConfig import ModuleConfig
from integration.IntegrationConfig import IntegrationConfig
from sample_representation.SampleRepresentationConfig import SampleRepresentationConfig
from preprocessing.PreprocessingConfig import PreprocessingConfig


min_version("6.0")
container: "docker://condaforge/mambaforge:latest"

configfile: "configs/outputs.yaml"
configfile: "configs/load_data/config.yaml"
configfile: "configs/exploration/config.yaml"

# params = pd.read_table('configs/modules.tsv',comment='#')
# params['submodules'] = params['submodules'].str.split(',')
# config = update_module_configs(config, params)

config_kwargs = {
    'batch_analysis': dict(
        config_params=['covariates', 'permute_covariates', 'n_permutations', 'sample'],
    ),
    'preprocessing': dict(
        config_params=['assemble', 'extra_hvgs'],
    ),
    'sample_representation': dict(
        parameters=workflow.source_path('sample_representation/params.tsv'),
        config_params=['methods', 'use_rep', 'var_mask', 'sample_key', 'norm_counts', 'raw_counts'],
        wildcard_names=['method', 'input_type', 'use_rep', 'var_mask'],
        rename_config_params={'methods': 'method'},
        explode_by=['method', 'use_rep', 'var_mask'],
    ),
    'split_data': dict(
        config_params=['key', 'values'],
        wildcard_names=['key', 'value'],
        rename_config_params={'values': 'value'},
        explode_by=['value'],
    ),
    'integration': dict(
        parameters=workflow.source_path('integration/params.tsv'),
        config_params=['methods', 'batch', 'label', 'var_mask'],
        wildcard_names=['method', 'batch', 'label', 'var_mask', 'output_type'],
        rename_config_params={'methods': 'method'},
        explode_by=['method', 'batch', 'label', 'var_mask'],
    ),
    'subset': dict(
        config_params=['strategy', 'n_cells', 'sample', 'label', 'per_sample'],
        explode_by='strategy',
        wildcard_names=['strategy'],
    ),
    'merge': dict(
        mandatory_wildcards=['dataset'],
    ),
    'collect': dict(
        mandatory_wildcards=['dataset'],
    ),
    'uncollect': dict(
        config_params=[
            'sep',
            'new_file_ids',
        ],
        wildcard_names=['new_file_id'],
        rename_config_params={'new_file_ids': 'new_file_id'},
        explode_by=['new_file_id'],
    ),
}

config_classes = {
    'integration': IntegrationConfig,
    'sample_representation': SampleRepresentationConfig,
    'preprocessing': PreprocessingConfig,
}

config['DATASETS'] = config.get('DATASETS', {})
default_datasets = config.get('defaults', {}).get('datasets', config['DATASETS'].keys())
for dataset, dataset_config in config['DATASETS'].items():
    for module_name in dataset_config.get('input', {}).keys():
        config = update_input_files_per_dataset(
            dataset=dataset,
            module_name=module_name,
            config=config,
            config_class_map=config_classes,
            config_kwargs=config_kwargs,
        )


# TODO move to data loader and exploration
if 'dataset_meta' in config:
    config['dataset_meta'] = Path(config['dataset_meta']).resolve()

os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'

envvars:
    'HDF5_USE_FILE_LOCKING'

# Import modules
module common:
    snakefile: "common/Snakefile"
    config: config

module load_data:
    snakefile: "load_data/Snakefile"
    config: config

module filtering:
    snakefile: "filter/Snakefile"
    config: config

module merge:
    snakefile: "merge/Snakefile"
    config: config

module collect:
    snakefile: "collect/Snakefile"
    config: config

module uncollect:
    snakefile: "uncollect/Snakefile"
    config: config

module exploration:
    snakefile: "exploration/Snakefile"
    config: config

module sample_representation:
    snakefile: "sample_representation/Snakefile"
    config: config

module batch_analysis:
    snakefile: "batch_analysis/Snakefile"
    config: config

module qc:
    snakefile: "qc/Snakefile"
    config: config

module doublets:
    snakefile: "doublets/Snakefile"
    config: config

module split_data:
    snakefile: "split_data/Snakefile"
    config: config

module subset:
    snakefile: "subset/Snakefile"
    config: config

module preprocessing:
    snakefile: "preprocessing/Snakefile"
    config: config

module clustering:
    snakefile: "clustering/Snakefile"
    config: config

module label_transfer:
    snakefile: "label_transfer/Snakefile"
    config: config

module label_harmonization:
    snakefile: "label_harmonization/Snakefile"
    config: config

module relabel:
    snakefile: "relabel/Snakefile"
    config: config

module integration:
    snakefile: "integration/Snakefile"
    config: config

module metrics:
    snakefile: "metrics/Snakefile"
    config: config

module marker_genes:
    snakefile: "marker_genes/Snakefile"
    config: config


use rule * from common as common_ *
use rule * from load_data as load_data_ *
use rule * from filtering as filter_ *
use rule * from merge as merge_ *
use rule * from collect as collect_ *
use rule * from uncollect as uncollect_ *
use rule * from preprocessing as preprocessing_ *
use rule * from clustering as clustering_ *
use rule * from exploration as exploration_ *
use rule * from sample_representation as sample_representation_ *
use rule * from batch_analysis as batch_analysis_ *
use rule * from doublets as doublets_ *
use rule * from qc as qc_ *
use rule * from split_data as split_data_ *
use rule * from subset as subset_ *
use rule * from label_transfer as label_transfer_ *
use rule * from label_harmonization as label_harmonization_ *
use rule * from relabel as relabel_ *
use rule * from integration as integration_ *
use rule * from metrics as metrics_ *
use rule * from marker_genes as marker_genes_ *


rule all:
    input: rules.dependency_graph.input
    default_target: True


def get_modules_from_config(config):
    modules = set()
    for dataset in config['DATASETS']:
        modules.update(config['DATASETS'][dataset]['input'].keys())
    return list(modules)

modules = get_modules_from_config(config)

rule dependency_graph:
    input:
        expand(
            rules.common_dependency_graph.input,
            images=config['images'],
            target=[f'{_module}_all' for _module in modules] + ['all']
        )
